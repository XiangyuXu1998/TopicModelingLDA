sugg.words = hunspell_suggest(bad.words)
sugg.words = unlist(lapply(sugg.words, function(x) x[1]))
word.list = as.data.frame(cbind(bad.words, sugg.words))
freq.word = count(changes_pd_skills_wd_tm, words)
freq.word = inner_join(freq.word, word.list, by = c(words = "bad.words"))
write.csv(freq.word, "misspelled_words.csv", row.names = FALSE)
bad.words = hunspell(comments_wd_unique)
bad.words = unique(unlist(bad.words))
length(bad.words)
# length is 18.
# Obtain the suggested words
sugg.words = hunspell_suggest(bad.words)
sugg.words = unlist(lapply(sugg.words, function(x) x[1]))
word.list = as.data.frame(cbind(bad.words, sugg.words))
freq.word = count(comments_wd_tm, words)
freq.word = inner_join(freq.word, word.list, by = c(words = "bad.words"))
write.csv(freq.word, "misspelled_words.csv", row.names = FALSE)
word.list = read.csv("misspelled_words_modified.csv", stringsAsFactors = FALSE)
bad.whole.words = paste0("\\b", word.list$words, "\\b")
sugg.words = word.list$sugg.words
# Replace misspelled words with suggested words
bad.whole.words = paste0("\\b", bad.words, "\\b")
comments = stri_replace_all_regex(comments, bad.whole.words, sugg.words, vectorize_all = FALSE)
# Change to lower case
comments = tolower(comments)
# Stem
stem_hunspell <- function(term) {
stems = hunspell_stem(term)[[1]]
if (length(stems) == 0) {
stem = NA
} else {
if (nchar(stems[[length(stems)]]) > 1)
stem = stems[[length(stems)]] else stem <- term
}
stem
}
word.list = count(comments_wd_tm, words)
## find the stems of the words
words.stem = lapply(word.list$words, stem_hunspell)
## save to file and manually check the words
stem.list = cbind(word.list, stem = unlist(words.stem))
stem.list = stem.list[!is.na(stem.list[, 3]) & stem.list[, 1] != stem.list[, 3], ]
write.csv(stem.list, "stem.list.csv", row.names = FALSE)
write.csv(stem.list, "stem_list.csv", row.names = FALSE)
stem.list = read.csv("stem_list_modified.csv", stringsAsFactors = FALSE)
## replace the words in the comments with stems
orig.words = paste0("\\b", stem.list[, 1], "\\b")
stem.words = stem.list[, 3]
comments <- stri_replace_all_regex(comments, orig.words, stem.words, vectorize_all = FALSE)
# Import stop list
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# Perplexity - 5-fold cross validation
k.topics <- 2:9
folding <- sample(1:5, nrow(comments.dtm), replace = TRUE)
runonce <- function(k, fold) {
testing.dtm <- which(folding == fold)
training.dtm <- which(folding != fold)
training.model <- LDA(comments.dtm[training.dtm, ], k = k)
test.model <- LDA(comments.dtm[testing.dtm, ], model = training.model, control = list(estimate.beta = FALSE))
perplexity(test.model)
}
res <- NULL
for (k in 2:9) {
for (fold in 1:5) {
res <- rbind(res, c(k, fold, runonce(k, fold)))
}
}
total.perp <- tapply(res[, 3], res[, 1], mean)
y = round(total.perp)
x = 2:9
plot(x, y, xlab = 'k', ylab = 'perplexity', type = 'b')
# LDA
# Find the ten words to each topic
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
stop.list
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*609
# LDA
# Find the ten words to each topic
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*606
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*604
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
# LDA
# Find the ten words to each topic
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stem.list = read.csv("stem_list_modified.csv", stringsAsFactors = FALSE)
## replace the words in the comments with stems
orig.words = paste0("\\b", stem.list[, 1], "\\b")
stem.words = stem.list[, 3]
comments <- stri_replace_all_regex(comments, orig.words, stem.words, vectorize_all = FALSE)
# Import stop list
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*602
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
data = read.csv("Comments.csv", header = FALSE)
comments = data[, 1]
# Get the unique word list
comments_wd_tm = unnest_tokens(data, words, 1)
comments_wd = comments_wd_tm[, "words"]
comments_wd_unique = unique(comments_wd)
length(comments_wd_unique)
# length is 813.
# Check for misspelled words
bad.words = hunspell(comments_wd_unique)
bad.words = unique(unlist(bad.words))
length(bad.words)
# length is 18.
# Obtain the suggested words
sugg.words = hunspell_suggest(bad.words)
sugg.words = unlist(lapply(sugg.words, function(x) x[1]))
word.list = as.data.frame(cbind(bad.words, sugg.words))
freq.word = count(comments_wd_tm, words)
freq.word = inner_join(freq.word, word.list, by = c(words = "bad.words"))
write.csv(freq.word, "misspelled_words.csv", row.names = FALSE)
# Check the suggested words manually and load the .csv file
word.list = read.csv("misspelled_words_modified.csv", stringsAsFactors = FALSE)
bad.whole.words = paste0("\\b", word.list$words, "\\b")
sugg.words = word.list$sugg.words
# Replace misspelled words with suggested words
bad.whole.words = paste0("\\b", bad.words, "\\b")
comments = stri_replace_all_regex(comments, bad.whole.words, sugg.words, vectorize_all = FALSE)
# Change to lower case
comments = tolower(comments)
# Stem
stem_hunspell <- function(term) {
stems = hunspell_stem(term)[[1]]
if (length(stems) == 0) {
stem = NA
} else {
if (nchar(stems[[length(stems)]]) > 1)
stem = stems[[length(stems)]] else stem <- term
}
stem
}
word.list = count(comments_wd_tm, words)
## find the stems of the words
words.stem = lapply(word.list$words, stem_hunspell)
## save to file and manually check the words
stem.list = cbind(word.list, stem = unlist(words.stem))
stem.list = stem.list[!is.na(stem.list[, 3]) & stem.list[, 1] != stem.list[, 3], ]
write.csv(stem.list, "stem_list.csv", row.names = FALSE)
stem.list = read.csv("stem_list_modified.csv", stringsAsFactors = FALSE)
## replace the words in the comments with stems
orig.words = paste0("\\b", stem.list[, 1], "\\b")
stem.words = stem.list[, 3]
comments <- stri_replace_all_regex(comments, orig.words, stem.words, vectorize_all = FALSE)
# Import stop list
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*596
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*598
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*596
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stem.list = read.csv("stem_list_modified.csv", stringsAsFactors = FALSE)
## replace the words in the comments with stems
orig.words = paste0("\\b", stem.list[, 1], "\\b")
stem.words = stem.list[, 3]
comments <- stri_replace_all_regex(comments, orig.words, stem.words, vectorize_all = FALSE)
# Import stop list
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*589
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
stop.list = read.csv("stop_words.csv", stringsAsFactors = FALSE)
stop.list[, 1] = tolower(stop.list[, 1])
# Remove stop words
whole.words = paste0("\\b", stop.list[, 1], "\\b")
comments = stri_replace_all_regex(comments, whole.words, " ", vectorize_all = FALSE)
## Create a corpus
comments.corpus = Corpus(VectorSource(comments))
## Text pre-processing
# Remove numbers and punctuation
comments.corpus = tm_map(comments.corpus, removeNumbers)
comments.corpus = tm_map(comments.corpus, removePunctuation)
# Remove stop words
comments.corpus = tm_map(comments.corpus, removeWords, stopwords('english'))
# Construct document term matrix
comments.dtm = DocumentTermMatrix(comments.corpus)
# A sample of DTM
as.matrix(comments.dtm)[1:5, 1:10]
# Remove low-frequency words and all-zero rows
# comments.dtm = removeSparseTerms(comments.dtm, sparse = 0.8)
unique_rows = unique(comments.dtm$i)
comments.dtm = comments.dtm[unique_rows, ]
dim(comments.dtm)
# dim: 20*584
comments.lda = LDA(comments.dtm, control = list(seed = 1), k = 2)
terms(comments.lda, 10)
comments.topics = tidy(comments.lda, matrix = "beta")
comments.topics
comments.terms = comments.topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>%
arrange(topic, -beta)
comments.terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") +
xlab("beta") + ylab("Terms") + scale_y_reordered()
comments.comments = tidy(comments.lda, matrix = "gamma")
comments.idx = comments.comments %>% group_by(topic) %>% top_n(5, gamma) %>%
ungroup() %>% arrange(topic, -gamma)
comments.idx = as.numeric(comments.idx$document)
data[comments.idx, 1]
pnorm(0.05)
qnorm(0.05)
qnorm(0.025)
n = 100
s = 69388.81
(3*n/s-3/500)/sqrt(1/s)
3*n/s-qnorm(0.025)*sqrt(1/s)
3*n/s+qnorm(0.025)*sqrt(1/s)
qnorm(0.5)
